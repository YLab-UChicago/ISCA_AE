import tensorflow as tf
from tensorflow.python.distribute import central_storage_strategy
from tensorflow.python.distribute import distribution_strategy_context as distribute_ctx
from tensorflow.python.distribute import parameter_server_strategy
from tensorflow.python.distribute import parameter_server_strategy_v2
from tensorflow.python.distribute import values as ds_values
from tensorflow.python.eager import backprop
from tensorflow.python.eager import context
from tensorflow.python.eager import monitoring
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.framework import tensor_util
from tensorflow.python.keras import backend
from tensorflow.python.keras import initializers
from tensorflow.python.keras.engine import base_layer_utils
from tensorflow.python.keras.optimizer_v2 import learning_rate_schedule
from tensorflow.python.keras.optimizer_v2 import utils as optimizer_utils
from tensorflow.python.keras.utils import generic_utils
from tensorflow.python.keras.utils import layer_utils
from tensorflow.python.keras.utils import tf_inspect
from tensorflow.python.keras.utils import tf_utils
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import gen_resource_variable_ops
from tensorflow.python.ops import gradients
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import variables as tf_variables
from tensorflow.python.saved_model import revived_types
from tensorflow.python.training.tracking import base as trackable
from tensorflow.python.util import nest
from tensorflow.python.util.tf_export import keras_export

def name_scope_only_in_function_or_graph(name):
  """Internal-only entry point for `name_scope*`.

  Enters a compat.v1.name_scope only when in a function or graph,
  not when running fully eagerly.

  Args:
    name: The name argument that is passed to the op function.

  Returns:
    `name_scope*` context manager.
  """
  if not context.executing_eagerly():
    return ops.name_scope_v1(name)
  else:
    return NullContextmanager()


class MyAdam(tf.keras.optimizers.Optimizer):
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, name="MyAdam", **kwargs):
        super(MyAdam, self).__init__(name, **kwargs)
        self._set_hyper("learning_rate", learning_rate)
        self._set_hyper("beta1", beta1)
        self._set_hyper("beta2", beta2)
        self._set_hyper("epsilon", epsilon)
        self.it = 1

    def _create_slots(self, var_list):
        for var in var_list:
            self.add_slot(var, "m")
            self.add_slot(var, "v")

    def _get_variable_index(self, var):
        # Convert the TPUDistributedVariable object to an integer index
        if isinstance(var, tf.Variable):
            return var._unique_id
        else:
            return var[0]._unique_id

    def _resource_apply_dense(self, grad, var):
        #var_index = self._get_variable_index(var)
        m = self.get_slot(var, "m")
        v = self.get_slot(var, "v")

        m_t = m.assign(self.beta1 * m + (1 - self.beta1) * grad)
        v_t = v.assign(self.beta2 * v + (1 - self.beta2) * tf.square(grad))

        #m_t_hat = m_t / (1 - tf.pow(self.beta1, self.iterations))
        m_t_hat = m_t / (1 - tf.pow(self.beta1, self.it))
        #v_t_hat = v_t / (1 - tf.pow(self.beta2, self.iterations))
        v_t_hat = v_t / (1 - tf.pow(self.beta2, self.it))
        self.it += 1

        update = -self.learning_rate * m_t_hat / (tf.sqrt(v_t_hat) + self.epsilon)
        var_update = var.assign_add(update)

        return tf.group(*[var_update, m_t, v_t])

    def _resource_apply_sparse(self, grad, var, indices):
        raise NotImplementedError("Sparse gradient updates are not supported.")


    def _resource_apply_and_check_dense(self, grad, var):
        m = self.get_slot(var, "m")
        v = self.get_slot(var, "v")

        m_t = m.assign(self.beta1 * m + (1 - self.beta1) * grad)
        v_t = v.assign(self.beta2 * v + (1 - self.beta2) * tf.square(grad))

        oob = tf.reduce_all(tf.greater(m_t, 100000))

        #m_t_hat = m_t / (1 - tf.pow(self.beta1, self.iterations))
        m_t_hat = m_t / (1 - tf.pow(self.beta1, self.it))
        #v_t_hat = v_t / (1 - tf.pow(self.beta2, self.iterations))
        v_t_hat = v_t / (1 - tf.pow(self.beta2, self.it))
        self.it += 1

        do_update = -self.learning_rate * m_t_hat / (tf.sqrt(v_t_hat) + self.epsilon)
        #no_update = tf.zeros(tf.shape(m))
        #update = tf.cond(oob, no_update, do_update)
        var_update = var.assign_add(do_update)
        return oob, tf.group(*[var_update, m_t, v_t])



    def apply_and_check_gradients(self, grads_and_vars, name=None):
        with tf.name_scope(name or self._name):
            oobs = []
            for grad, var in grads_and_vars:
                oob, _ = self._resource_apply_and_check_dense(grad, var)
                oobs.append(oob)
            return tf.reduce_all(oobs)


    def reset(self, grads, var_list):
        self.it -= 0
        for var,grad in zip(var_list, grads):
            m = self.get_slot(var, "m")
            v = self.get_slot(var, "v")
            m_t_hat = m / (1 - tf.pow(self.beta1, self.it))
            v_t_hat = v / (1 - tf.pow(self.beta2, self.it))

            m_t = m.assign((m - (1-self.beta1) * grad) / self.beta1)
            v_t = v.assign((v - (1-self.beta2) * tf.square(grad)) / self.beta2)
            update = self.learning_rate * m_t_hat / (tf.sqrt(v_t_hat) + self.epsilon)
            var_update = var.assign_add(update)
        return tf.group(*[var_update, m_t, v_t])

    def get_config(self):
        config = super(MyAdam, self).get_config()
        config.update({
            "learning_rate": self._serialize_hyperparameter("learning_rate"),
            "beta1": self._serialize_hyperparameter("beta1"),
            "beta2": self._serialize_hyperparameter("beta2"),
            "epsilon": self._serialize_hyperparameter("epsilon")
        })
        return config


  def apply_and_check_gradients(self,
                      grads_and_vars,
                      name=None,
                      experimental_aggregate_gradients=True):

    grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)
    var_list = [v for (_, v) in grads_and_vars]

    with ops.name_scope_v2(self._name):
      # Create iteration if necessary.
      with ops.init_scope():
        self._create_all_weights(var_list)

      if not grads_and_vars:
        # Distribution strategy does not support reducing an empty list of
        # gradients
        return control_flow_ops.no_op()

      if distribute_ctx.in_cross_replica_context():
        raise RuntimeError(
            "`apply_gradients() cannot be called in cross-replica context. "
            "Use `tf.distribute.Strategy.run` to enter replica "
            "context.")

      strategy = distribute_ctx.get_strategy()
      if (not experimental_aggregate_gradients and strategy and
          isinstance(strategy,
                     (parameter_server_strategy.ParameterServerStrategyV1,
                      parameter_server_strategy_v2.ParameterServerStrategyV2,
                      central_storage_strategy.CentralStorageStrategy,
                      central_storage_strategy.CentralStorageStrategyV1))):
        raise NotImplementedError(
            "`experimental_aggregate_gradients=False is not supported for "
            "ParameterServerStrategy and CentralStorageStrategy")

      apply_state = self._prepare(var_list)
      if experimental_aggregate_gradients:
        grads_and_vars = self._transform_unaggregated_gradients(grads_and_vars)
        grads_and_vars = self._aggregate_gradients(grads_and_vars)
      grads_and_vars = self._transform_gradients(grads_and_vars)

      if optimizer_utils.strategy_supports_no_merge_call():
        return self._distributed_apply(strategy, grads_and_vars, name,
                                       apply_state)
      else:
        return distribute_ctx.get_replica_context().merge_call(
            functools.partial(self._distributed_apply, apply_state=apply_state),
            args=(grads_and_vars,),
            kwargs={
                "name": name,
            })


    def _distributed_apply(self, distribution, grads_and_vars, name, apply_state):
        """`apply_gradients` using a `DistributionStrategy`."""

        def apply_grad_to_update_var(var, grad):
            """Apply gradient to variable."""
            if isinstance(var, ops.Tensor):
                raise NotImplementedError("Trying to update a Tensor ", var)

            apply_kwargs = {}
            if isinstance(grad, ops.IndexedSlices):
                if var.constraint is not None:
                    raise RuntimeError(
                        "Cannot use a constraint function on a sparse variable.")
                if "apply_state" in self._sparse_apply_args:
                    apply_kwargs["apply_state"] = apply_state
                return self._resource_apply_sparse_duplicate_indices(
                    grad.values, var, grad.indices, **apply_kwargs)

            if "apply_state" in self._dense_apply_args:
                apply_kwargs["apply_state"] = apply_state
            # MOD
            #update_op = self._resource_apply_dense(grad, var, **apply_kwargs)
            oob, update_op = self._resource_apply_and_check_dense(grad, var, **apply_kwargs)
            if var.constraint is not None:
                with ops.control_dependencies([update_op]):
                    return oob, var.assign(var.constraint(var))
            else:
                return oob, update_op

        eagerly_outside_functions = ops.executing_eagerly_outside_functions()
        update_ops = []
        with name_scope_only_in_function_or_graph(name or self._name):
            for grad, var in grads_and_vars:
                # Colocate the update with variables to avoid unnecessary communication
                # delays. See b/136304694.
                with distribution.extended.colocate_vars_with(var):
                    with name_scope_only_in_function_or_graph(
                        "update" if eagerly_outside_functions else "update_" +
                        var.op.name):
                        update_op = distribution.extended.update(
                            var, apply_grad_to_update_var, args=(grad,), group=False)
                        if distribute_ctx.in_cross_replica_context():
                            # In cross-replica context, extended.update returns a list of
                            # update ops from all replicas (group=False).
                            update_ops.extend(update_op)
                        else:
                            # In replica context, extended.update return the single update op
                            # of current replica.
                            update_ops.append(update_op)

            any_symbolic = any(isinstance(i, ops.Operation) or
                         tf_utils.is_symbolic_tensor(i) for i in update_ops)
            if not context.executing_eagerly() or any_symbolic:
                # If the current context is graph mode or any of the update ops are
                # symbolic then the step update should be carried out under a graph
                # context. (eager updates execute immediately)
                with backend._current_graph(update_ops).as_default():  # pylint: disable=protected-access
                    with ops.control_dependencies([control_flow_ops.group(update_ops)]):
                        return self._iterations.assign_add(1, read_value=False)

            return self._iterations.assign_add(1)

